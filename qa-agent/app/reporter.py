"""
Report Generator - Creates Markdown reports from test results
"""
import os
from datetime import datetime
from typing import Dict, Any, List


class ReportGenerator:
    def __init__(self, reports_dir: str = "reports"):
        self.reports_dir = reports_dir
        os.makedirs(reports_dir, exist_ok=True)

    def generate(
        self,
        test_id: str,
        url: str,
        objective: str,
        results: Dict[str, Any]
    ) -> str:
        """Generate a Markdown report from test results"""

        timestamp = datetime.now()
        filename = f"report_{test_id}_{timestamp.strftime('%Y%m%d_%H%M%S')}.md"
        filepath = os.path.join(self.reports_dir, filename)

        # Calculate stats
        passed = results.get("passed", 0)
        failed = results.get("failed", 0)
        total_steps = len(results.get("steps_completed", []))
        errors = results.get("errors", [])
        screenshots = results.get("screenshots", [])

        # Determine overall status
        if failed == 0 and passed > 0:
            status_emoji = "✅"
            status_text = "PASSED"
        elif failed > 0 and passed > 0:
            status_emoji = "⚠️"
            status_text = "PARTIAL"
        else:
            status_emoji = "❌"
            status_text = "FAILED"

        # Build report
        report = f"""# QA Test Report

## Summary
| | |
|---|---|
| **Test ID** | `{test_id}` |
| **Target URL** | {url} |
| **Objective** | {objective} |
| **Status** | {status_emoji} {status_text} |
| **Timestamp** | {timestamp.strftime('%Y-%m-%d %H:%M:%S')} |

## Results Overview

| Metric | Count |
|--------|-------|
| Total Steps | {total_steps} |
| Passed | {passed} |
| Failed | {failed} |
| Screenshots | {len(screenshots)} |

---

## Execution Log

"""
        # Add step-by-step log
        steps = results.get("steps_completed", [])
        for i, step in enumerate(steps, 1):
            tool = step.get("tool", "unknown")
            inputs = step.get("input", {})
            result = step.get("result", "No result")

            # Determine step status
            if "SUCCESS" in str(result):
                step_status = "✅"
            elif "ERROR" in str(result) or "FAILURE" in str(result):
                step_status = "❌"
            else:
                step_status = "➡️"

            report += f"""### Step {i}: {tool} {step_status}

**Action:** `{tool}`
**Parameters:** `{inputs}`
**Result:** {result}

---

"""

        # Add errors section
        if errors:
            report += """## Errors Found

| Tool | Error | Details |
|------|-------|---------|
"""
            for error in errors:
                report += f"| {error.get('tool', 'N/A')} | {error.get('error', 'Unknown')[:50]} | {str(error.get('inputs', {}))[:50]} |\n"

            report += "\n---\n\n"

        # Add screenshots section
        if screenshots:
            report += """## Screenshots

"""
            for screenshot in screenshots:
                name = os.path.basename(screenshot)
                report += f"- [{name}]({screenshot})\n"

            report += "\n---\n\n"

        # Add recommendations
        report += """## Recommendations

"""
        if errors:
            report += """### Issues to Address

"""
            for error in errors:
                report += f"- **{error.get('tool', 'Action')}** failed: {error.get('error', 'Unknown error')}\n"

        if status_text == "PASSED":
            report += """
All tests passed successfully. The tested flow is working as expected.

### Suggested Next Steps
- Set up automated daily testing
- Add more test scenarios (edge cases, error handling)
- Monitor for regressions
"""
        elif status_text == "PARTIAL":
            report += """
Some tests passed but issues were found. Review the errors above.

### Suggested Next Steps
- Fix the reported issues
- Re-run tests after fixes
- Add regression tests for fixed bugs
"""
        else:
            report += """
Critical issues found. The flow may be broken.

### Suggested Next Steps
- Investigate the errors immediately
- Check server logs and error monitoring
- Hot-fix critical issues
"""

        # Footer
        report += f"""
---

*Report generated by QA Testing Agent*
*Test ID: {test_id}*
*Generated: {timestamp.isoformat()}*
"""

        # Write report
        with open(filepath, "w") as f:
            f.write(report)

        return filepath

    def generate_daily_summary(
        self,
        tests: List[Dict[str, Any]]
    ) -> str:
        """Generate a daily summary report of all tests"""

        timestamp = datetime.now()
        filename = f"daily_summary_{timestamp.strftime('%Y%m%d')}.md"
        filepath = os.path.join(self.reports_dir, filename)

        total_tests = len(tests)
        passed_tests = len([t for t in tests if t.get("status") == "completed" and not t.get("summary", {}).get("failed")])
        failed_tests = total_tests - passed_tests

        report = f"""# Daily QA Summary - {timestamp.strftime('%Y-%m-%d')}

## Overview

| Metric | Value |
|--------|-------|
| Total Tests Run | {total_tests} |
| Passed | {passed_tests} |
| Failed | {failed_tests} |
| Success Rate | {(passed_tests/total_tests*100) if total_tests > 0 else 0:.1f}% |

## Test Results

| Test ID | URL | Objective | Status |
|---------|-----|-----------|--------|
"""
        for test in tests:
            status = "✅" if test.get("status") == "completed" and not test.get("summary", {}).get("failed") else "❌"
            report += f"| {test.get('test_id', 'N/A')} | {test.get('url', 'N/A')[:30]} | {test.get('objective', 'N/A')} | {status} |\n"

        report += f"""

---

*Daily Summary generated by QA Testing Agent*
*Date: {timestamp.strftime('%Y-%m-%d')}*
"""

        with open(filepath, "w") as f:
            f.write(report)

        return filepath
